{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMTJ+XxTuwfEHWPl9PQ8KfC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/raw-fun/AI/blob/main/Copy_of_web_scrap_update.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M1S6aUzDTwPu"
      },
      "outputs": [],
      "source": [
        "# ‡ßß. ‡¶™‡ßç‡¶∞‡ßã-‡¶≤‡ßá‡¶≠‡ßá‡¶≤ ‡¶∏‡ßá‡¶ü‡¶Ü‡¶™\n",
        "!pip install -q playwright pandas\n",
        "!playwright install chromium\n",
        "!playwright install-deps chromium\n",
        "\n",
        "import asyncio\n",
        "import re\n",
        "import random\n",
        "import pandas as pd\n",
        "from playwright.async_api import async_playwright\n",
        "from urllib.parse import urljoin, urlparse\n",
        "import datetime\n",
        "\n",
        "print(\"üöÄ ‡¶Ö‡ßç‡¶Ø‡¶æ‡¶°‡¶≠‡¶æ‡¶®‡ßç‡¶∏‡¶° ‡¶´‡¶∞‡ßá‡¶®‡¶∏‡¶ø‡¶ï ‡¶á‡¶û‡ßç‡¶ú‡¶ø‡¶® ‡¶™‡ßç‡¶∞‡¶∏‡ßç‡¶§‡ßÅ‡¶§‡•§\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import asyncio\n",
        "import re\n",
        "import random\n",
        "from urllib.parse import urljoin, urlparse\n",
        "\n",
        "class DeepLinkForensicFinal:\n",
        "    def __init__(self, root_url, max_pages=50):\n",
        "        self.root_url = root_url.rstrip('/')\n",
        "        self.max_pages = max_pages\n",
        "        self.nodes = [] #\n",
        "        self.visited = set()\n",
        "        self.queue = [root_url]\n",
        "\n",
        "    def is_valid_page(self, url):\n",
        "        \"\"\"‡¶∂‡ßÅ‡¶ß‡ßÅ‡¶Æ‡¶æ‡¶§‡ßç‡¶∞ ‡¶Ü‡¶∏‡¶≤ ‡¶™‡ßá‡¶ú‡¶ó‡ßÅ‡¶≤‡ßã ‡¶∏‡ßç‡¶ï‡ßç‡¶Ø‡¶æ‡¶® ‡¶ï‡¶∞‡¶¨‡ßá, ‡¶´‡¶æ‡¶á‡¶≤ ‡¶®‡ßü\"\"\"\n",
        "        parsed = urlparse(url)\n",
        "        # ‡¶¨‡¶æ‡¶¶ ‡¶¶‡ßá‡¶ì‡ßü‡¶æ‡¶∞ ‡¶ú‡¶®‡ßç‡¶Ø ‡¶´‡¶æ‡¶á‡¶≤ ‡¶è‡¶ï‡ßç‡¶∏‡¶ü‡ßá‡¶®‡¶∂‡¶® ‡¶≤‡¶ø‡¶∏‡ßç‡¶ü\n",
        "        excluded_ext = ('.css', '.js', '.ico', '.png', '.jpg', '.jpeg', '.svg', '.gif', '.pdf', '.woff', '.woff2')\n",
        "        # ‡¶Ø‡¶¶‡¶ø ‡¶á‡¶â‡¶Ü‡¶∞‡¶è‡¶≤‡¶ü‡¶ø ‡¶è‡¶á ‡¶è‡¶ï‡ßç‡¶∏‡¶ü‡ßá‡¶®‡¶∂‡¶®‡¶ó‡ßÅ‡¶≤‡ßã‡¶∞ ‡¶ï‡ßã‡¶®‡ßã‡¶ü‡¶ø ‡¶¶‡¶ø‡ßü‡ßá ‡¶∂‡ßá‡¶∑ ‡¶π‡ßü, ‡¶§‡¶¨‡ßá ‡¶∏‡ßá‡¶ü‡¶ø ‡¶¨‡¶æ‡¶§‡¶ø‡¶≤\n",
        "        if parsed.path.lower().endswith(excluded_ext):\n",
        "            return False\n",
        "        # ‡¶∂‡ßÅ‡¶ß‡ßÅ‡¶Æ‡¶æ‡¶§‡ßç‡¶∞ ‡¶∏‡¶∞‡¶ï‡¶æ‡¶∞‡¶ø ‡¶°‡ßã‡¶Æ‡ßá‡¶á‡¶® ‡¶ö‡ßá‡¶ï\n",
        "        return \".gov.bd\" in parsed.netloc\n",
        "\n",
        "    async def extract_links_clean(self, page, current_url):\n",
        "        \"\"\"‡¶Ö‡¶™‡ßç‡¶∞‡ßü‡ßã‡¶ú‡¶®‡ßÄ‡ßü ‡¶´‡¶æ‡¶á‡¶≤ ‡¶¨‡¶æ‡¶¶ ‡¶¶‡¶ø‡ßü‡ßá ‡¶∂‡ßÅ‡¶ß‡ßÅ‡¶Æ‡¶æ‡¶§‡ßç‡¶∞ ‡¶™‡ßá‡¶ú ‡¶≤‡¶ø‡¶ô‡ßç‡¶ï ‡¶∏‡¶Ç‡¶ó‡ßç‡¶∞‡¶π ‡¶ï‡¶∞‡¶¨‡ßá\"\"\"\n",
        "        try:\n",
        "            content = await page.content()\n",
        "            # ‡¶∞‡ßá‡¶ú‡ßá‡¶ï‡ßç‡¶∏ ‡¶¨‡ßç‡¶Ø‡¶¨‡¶π‡¶æ‡¶∞ ‡¶ï‡¶∞‡ßá ‡¶∏‡¶¨ ‡¶≤‡¶ø‡¶ô‡ßç‡¶ï ‡¶¨‡ßá‡¶∞ ‡¶ï‡¶∞‡¶æ\n",
        "            links = re.findall(r'href=[\\'\"]?([^\\'\" >]+)', content)\n",
        "\n",
        "            added = 0\n",
        "            for link in set(links):\n",
        "                full_url = urljoin(current_url, link).split('#')[0].split('?')[0].rstrip('/')\n",
        "\n",
        "                if full_url not in self.visited and self.is_valid_page(full_url):\n",
        "                    if len(self.visited) + len(self.queue) < self.max_pages + 50:\n",
        "                        self.queue.append(full_url)\n",
        "                        added += 1\n",
        "            return added\n",
        "        except:\n",
        "            return 0\n",
        "\n",
        "    async def run(self):\n",
        "        async with async_playwright() as p:\n",
        "            browser = await p.chromium.launch(headless=True, args=[\"--disable-blink-features=AutomationControlled\"])\n",
        "            context = await browser.new_context(\n",
        "                user_agent=\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36\"\n",
        "            )\n",
        "\n",
        "            print(f\"üïµÔ∏è ‡¶Æ‡¶ø‡¶∂‡¶® ‡¶∏‡ßç‡¶ü‡ßç‡¶Ø‡¶æ‡¶ü‡¶æ‡¶∏: {self.root_url} ‡¶è ‡¶ï‡ßç‡¶≤‡¶ø‡¶® ‡¶∏‡ßç‡¶ï‡ßç‡¶Ø‡¶æ‡¶® ‡¶∂‡ßÅ‡¶∞‡ßÅ...\")\n",
        "            print(\"-\" * 75)\n",
        "\n",
        "            while self.queue and len(self.nodes) < self.max_pages:\n",
        "                url = self.queue.pop(0)\n",
        "                if url in self.visited: continue\n",
        "                self.visited.add(url)\n",
        "\n",
        "                page = await context.new_page()\n",
        "                # ‡¶∞‡¶ø‡¶∏‡ßã‡¶∞‡ßç‡¶∏ ‡¶¨‡ßç‡¶≤‡¶ï‡¶ø‡¶Ç\n",
        "                await page.route(\"**/*.{css,js,png,jpg,ico,woff,svg}\", lambda r: r.abort())\n",
        "\n",
        "                success = False\n",
        "                retries = 2\n",
        "\n",
        "                while retries > 0 and not success:\n",
        "                    try:\n",
        "                        print(f\"üîç [{len(self.nodes)+1}] ‡¶≠‡¶ø‡¶ú‡¶ø‡¶ü: {url[:60]}...\", end=\" \")\n",
        "\n",
        "                        # ‡¶®‡ßá‡¶≠‡¶ø‡¶ó‡ßá‡¶∂‡¶® ‡¶∏‡ßá‡¶ü‡¶ø‡¶Ç‡¶∏\n",
        "                        response = await page.goto(url, wait_until=\"domcontentloaded\", timeout=45000)\n",
        "                        await asyncio.sleep(2) # ‡¶∞‡ßá‡¶®‡ßç‡¶°‡¶æ‡¶∞‡¶ø‡¶Ç ‡¶¨‡¶ø‡¶∞‡¶§‡¶ø\n",
        "\n",
        "                        status = response.status if response else 200\n",
        "\n",
        "                        if status == 521 or status == 504:\n",
        "                            print(f\"‚è≥ [Status {status}] ‡¶™‡ßÅ‡¶®‡¶∞‡¶æ‡ßü ‡¶ö‡ßá‡¶∑‡ßç‡¶ü‡¶æ ‡¶ï‡¶∞‡¶æ ‡¶π‡¶ö‡ßç‡¶õ‡ßá...\", end=\" \")\n",
        "                            retries -= 1\n",
        "                            await asyncio.sleep(5)\n",
        "                            continue\n",
        "\n",
        "                        self.nodes.append({\"url\": url, \"status\": status, \"domain\": urlparse(url).netloc})\n",
        "                        print(f\"‚úÖ [{status}]\")\n",
        "\n",
        "                        # ‡¶≤‡¶ø‡¶ô‡ßç‡¶ï ‡¶∏‡¶Ç‡¶ó‡ßç‡¶∞‡¶π\n",
        "                        new_nodes = await self.extract_links_clean(page, url)\n",
        "                        if new_nodes > 0: print(f\"   ‚àü üß¨ {new_nodes}‡¶ü‡¶ø ‡¶≠‡ßç‡¶Ø‡¶æ‡¶≤‡¶ø‡¶° ‡¶®‡ßã‡¶° ‡¶ï‡¶ø‡¶â‡¶§‡ßá ‡¶Ø‡ßÅ‡¶ï‡ßç‡¶§‡•§\")\n",
        "\n",
        "                        success = True\n",
        "                    except Exception:\n",
        "                        retries -= 1\n",
        "                        if retries == 0:\n",
        "                            print(\"‚ùå ‡¶∏‡ßç‡¶ï‡¶ø‡¶™‡¶° (‡¶ü‡¶æ‡¶á‡¶Æ‡¶Ü‡¶â‡¶ü)\")\n",
        "                            self.nodes.append({\"url\": url, \"status\": \"Timed Out\", \"domain\": urlparse(url).netloc})\n",
        "\n",
        "                await page.close()\n",
        "                await asyncio.sleep(random.uniform(1, 2)) # ‡¶π‡¶ø‡¶â‡¶Æ‡ßç‡¶Ø‡¶æ‡¶® ‡¶°‡¶ø‡¶≤‡ßç\n",
        "\n",
        "            await browser.close()\n",
        "            return self.nodes"
      ],
      "metadata": {
        "id": "K7HrsI4LT7a0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ‡¶Æ‡¶ø‡¶∂‡¶® ‡¶™‡ßç‡¶Ø‡¶æ‡¶∞‡¶æ‡¶Æ‡¶ø‡¶ü‡¶æ‡¶∞\n",
        "TARGET = \"https://bangladesh.gov.bd\" #@param {type:\"string\"}\n",
        "TOTAL_PAGES = 1000 #@param {type:\"integer\"}\n",
        "\n",
        "async def start_clean_mission():\n",
        "    engine = DeepLinkForensicFinal(TARGET, max_pages=TOTAL_PAGES)\n",
        "    results = await engine.run()\n",
        "\n",
        "    df = pd.DataFrame(results)\n",
        "    if df.empty: return\n",
        "\n",
        "    print(\"\\n\" + \"‚ñà\"*75)\n",
        "    print(\"              üèÅ FINAL CLEAN FORENSIC REPORT\")\n",
        "    print(\"‚ñà\"*75)\n",
        "    print(f\"üîπ ‡¶Æ‡ßã‡¶ü ‡¶™‡ßç‡¶∞‡¶∏‡ßá‡¶∏‡¶° ‡¶á‡¶â‡¶Ü‡¶∞‡¶è‡¶≤  : {len(df)}\")\n",
        "    print(f\"üîπ ‡¶∏‡¶´‡¶≤ ‡¶ï‡¶æ‡¶®‡ßá‡¶ï‡¶∂‡¶® (200)   : {len(df[df['status'] == 200])}\")\n",
        "    print(f\"üîπ ‡¶°‡ßã‡¶Æ‡ßá‡¶á‡¶® ‡¶°‡¶æ‡¶á‡¶≠‡¶æ‡¶∞‡¶∏‡¶ø‡¶ü‡¶ø    : {df['domain'].nunique()} ‡¶ü‡¶ø\")\n",
        "    print(\"-\" * 75)\n",
        "\n",
        "    display(df.head(20))\n",
        "    df.to_csv(\"clean_forensic_results.csv\", index=False)\n",
        "\n",
        "await start_clean_mission()"
      ],
      "metadata": {
        "id": "SLeVC-UyT9nD"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}